---
title: Autonomous exploration
subtitle: Deep Reinforcement Learning
layout: default
modal-id: 10
date: 2019-01-26
img: jackal-build.jpg
thumbnail: jackal-build-thumbnail.jpg
alt: image-alt
project-date: December 2019
category: Web Development
description: <div align="center"><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/YdH4bE5dZ6M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div><br><br>To see more you can visit <a href="https://github.com/robo-jordo/jackal_self_exploration">Jackal self exploration</a> <h3>Overview</h3> This project aims to use reinforcement learning on a Gazebo simulation of the Jackal with the end goal of transferring the learned policy to the real robot. A large part of this project was writing a library to interface with Gazebo in a clean and easy manner through ROS. Once this framework is set up there are multiple autonomous navigation and exploration goals that can be tackled with various algorithms. One of these goals is to investigate how policies change based on changing the sight range of the Jackal, this learning aim is in line with the interests of a sponsoring lab at Northwestern University. <br> <br> <h3>Gazebo and ROS</h3> ROS and Gazebo have been used to allow the Jackal to learn in an environment that provides easy restarts and no damage to the physical robot. ClearPaths simulation repositories for the Jackal were cloned and built from source. Some of the repositories had to be forked so that small adjustments could be made in order to better fit with the real world Northwestern specific model of the Jackal. <!--The most important of these changes was changing the simulation LIDAR in in the  <a href="https://github.com/robo-jordo/jackal_melodic_bringup">LMS1xx repository</a> to be a 360 degree LIDAR to match the laser scan produced by condensing the 3D scan on the real robot. --> <h3>State Observation forms</h3> <h6>Single environments</h6> In order to explore a single environment, (x,y) co-ordinates can be used as state information with "new map information" learned at each step as a reward. This will allow for learning a policy specific to that one environment. <h6>General environments</h6> Using RL to learn policies for general environments is not trivial and is not common practice. There are better algorithms and heuristics for exploring general environments, However reinforcement learning was used for this project with the hopes of exploring the difference that range of sight makes and to get experience using reinforcement learning. <br><br> In order to learn "general" policies, any state information that is specific to a single environment (e.g x,y co-ordinates) should not be used. For this reason environment agnostic states have been chosen. The environment agnostic states that can be used are&#58; <br><br> &bull;The full 3D point cloud <br> &bull; Condensing the point cloud into a flat Laser scan<br> &bull; Segmenting the laser scan and using the mean/min/max <br> or some other metric of each segment<br> &bull; Segmenting the laser scan and binning the mean/min/max/other <br> metric of each segment into discrete values<br> &bull; Using a map of the robots current understanding of its surroundings. <br><br> Examples of the last three options can be seen in the figures below. <img src="img/portfolio/lidar_segs.png" alt="Examples of segmented and discretized LIDAR models."> <br> Examples of segmented and discretized LIDAR models. <br><br> <img src="img/portfolio/cnn_conversion.png" alt="Part of environment mapped by the robot on the right. The image representation to be fed to a CNN on the left."> <br> Part of environment mapped by the robot on the right. The image representation to be fed to a CNN on the left. <br> <h3>Algorithms</h3> <h5>SARSA</h5> The first algorithm implemented was SARSA, this is owing to the fact that the problem of learning how to autonomously navigate an arbitrary environment is a model free problem. The SARSA implementation would require a the number of states to be relatively small and discrete. This is possible if learning on one environment only using (x, y) co-ordinates as a state. However to be able to use the LIDAR readings this is infeasible as there are too many states and they are possibly continuous in value. To accommodate this Deep reinforcement learning was implemented. <h5>Deep RL</h5> Deep reinforcement learning can solve the problem of very large or continuous state spaces. Deep rl allows for approximation of the Q values of state, action pairs rather than specifically calculating and storing them in a table. This causes the algorithm to be feasible in terms of memory and it also makes the process of reinforcement learning more sample efficient (The agent no longer has to visit every state and perform each action multiple times.) <h5>Target networks and memory replay</h5> Deep mind has produced much greater results for deep reinforcement learning by using some stabilization strategies in learning. Two of these strategies have been implemented. <h6>Target network</h6> This method makes use of a second neural network with the exact same structure as the original one. The original neural network is updated every time however the weights from this network are only copied to the second network periodically. The second network is used to calculate the Q-values of the next state in the update step. This provides for more stable learning as the target Q-values are not continuously changing while the policy is being while learning is attempting to match it. <h6>Memory replay</h6> Memory replay is a strategy that stores the robots previous experiences in a memory buffer. The neural network is then periodically trained on random batches from this buffer. This allows the robot to form good policies about the general task rather only learning a policy that fits well to what it is currently experiencing. This is helpful here since the robot may experience different structure in different parts of the environment. <h3>Learning goals</h3> The learning goals of this project can be easily adjusted within the reinforcement learning framework that has been set up by changing&#58; <h6> &bull; the reward function</h6> The reward function used can be easily tailored to include goals of the algorithm that are measurable in the form of a scalar value. The policy that the algorithm develops should then develop in a way to maximize this scalar value. If using multiple different rewards to create a single scalar each contributor can be weighted based on importance. An example of a reward structure could be&#58; <br>  <br> &bull; A negative scalar value (e.g. -1) can be given for each time step where no new information is learned to encourage the robot to explore. <br> &bull; A negative scalar value (e.g. -10) can be given each time the robot makes a move that would result in it crashing. <br> &bull; A positive scalar reward (e.g. +5) can be given for each time step where new map information is learned. This can be a constant vale or it can be proportional to the amount of new information learned. <br> <br> <h6> &bull; the state representation </h6> The state representation can change the learning goal of the algorithm as discussed in the General Environments section above. <br> x,y co-ordinates can be used to create a policy based on the structure of a single environment, while robot observations like LIDAR scans or a map of the robots current understanding of the world can be used to create policies based on what the robot can observe. This can be further explored by limiting the range of what the robot can observe. <h6> &bull; the neural network structure</h6> This project makes use of Keras to implement neural networks, this makes it simple to adjust not only the size and shape of the neural network but also the types of layers used. <br>Recurrent layers can be added to allow for the temporal aspects of the states and actions to be included in the model. <br>Convolutional layers can be added in order to be able to feed the network spatial/image data such as the map images created by the SLAM algorithm being used on the robot. <h3>Results</h3> Various learning aims were tried while testing the framework. These included&#58; <br>&bull; trying to learn a simple obstacle avoidance policy based on segmented LIDAR data with a reward policy of -1 for bumping an object and 1 for not bumping an object. <br>&bull; Learning a policy for a single environment using the (x,y) values as state and new map information learned as a reward metric with negative reward per time step and negative reward for collisions.<br>&bull; Using a Cost map as state input to a CNN<br>  <h3>Future learning goals</h3> Owing to the speed at which the Jackal can be simulated in Gazebo trials were limited in the time provided. While the results are positive better results could definitely be achieved by tuning the hyperparamters in the algorithm, Given more time the project could also possibly be developed to meet the goals of the following very interesting applications to the algorithm <br>&bull; Using camera frames from the front of the robot as input to a CNN with a reward structure that accounts for collisions and new map information gained <br>&bull; Artificially limiting LIDAR range to see difference in optimal policies based on range of sight. <h3>Problems encountered and outtakes</h3> Stabilization of the learning process was a hard problem to identify and solve. There are no tell tale signs of this instability and I only found that the process was unstable by trying my algorithm on a much simpler problem (the OpenAI gym cartpole problem). The stabilization techniques used don't seem as though they can hurt the learning process so they might as well be included. <br><br>Finding an effective state representation and algorithm structure for learning policies to navigate general environments is not trivial and I would like to put more work into it. The use of the map produced by the slam algorithm with a CNN seems promising. 

---
